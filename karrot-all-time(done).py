import json, os, csv, time
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, TimeoutException, WebDriverException
import urllib.parse
import re

# â”€â”€â”€ ì„œìš¸ì‹œ êµ¬ ë¦¬ìŠ¤íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SEOUL_DISTRICTS = [
    "ê°•ë‚¨êµ¬", "ê°•ë™êµ¬", "ê°•ë¶êµ¬", "ê°•ì„œêµ¬", "ê´€ì•…êµ¬", "ê´‘ì§„êµ¬", "êµ¬ë¡œêµ¬", "ê¸ˆì²œêµ¬",
    "ë…¸ì›êµ¬", "ë„ë´‰êµ¬", "ë™ëŒ€ë¬¸êµ¬", "ë™ì‘êµ¬", "ë§ˆí¬êµ¬", "ì„œëŒ€ë¬¸êµ¬", "ì„œì´ˆêµ¬", "ì„±ë™êµ¬",
    "ì„±ë¶êµ¬", "ì†¡íŒŒêµ¬", "ì–‘ì²œêµ¬", "ì˜ë“±í¬êµ¬", "ìš©ì‚°êµ¬", "ì€í‰êµ¬", "ì¢…ë¡œêµ¬", "ì¤‘êµ¬", "ì¤‘ë‘êµ¬"
]

# â”€â”€â”€ ìºì‹œëœ JSON ì½ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with open("categories.json","r",encoding="utf-8") as f:
    categories = json.load(f)

class KarrotCrawler:
    def __init__(self):
        opts = uc.ChromeOptions()
        opts.add_argument('--no-sandbox')
        opts.add_argument('--disable-dev-shm-usage')
        opts.add_argument('--disable-blink-features=AutomationControlled')
        opts.add_argument('--disable-infobars')
        opts.add_argument('--disable-extensions')
        opts.add_argument('--start-maximized')
        opts.add_argument('--disable-gpu')
        opts.add_argument('--window-size=1920,1080')
        opts.add_argument('--ignore-certificate-errors')
        opts.add_argument('--allow-running-insecure-content')
        opts.add_argument('--disable-web-security')
        opts.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        self.driver = uc.Chrome(options=opts)
        self.wait   = WebDriverWait(self.driver, 15)

    def safe_get(self, url, max_retries=3):
        """ì•ˆì „í•˜ê²Œ í˜ì´ì§€ ë¡œë“œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        for attempt in range(max_retries):
            try:
                self.driver.get(url)
                time.sleep(2)
                return True
            except WebDriverException as e:
                print(f"âš ï¸ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {str(e)}")
                if attempt < max_retries - 1:
                    time.sleep(2)
                    continue
                return False
        return False

    def get_district_dongs(self, district):
        """êµ¬ ì´ë¦„ìœ¼ë¡œ ë™ë„¤ ì •ë³´ ìˆ˜ì§‘"""
        print(f"\nâ–¶ [{district}] ë™ë„¤ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘")
        
        # í™ˆí˜ì´ì§€ë¡œ ì´ë™
        if not self.safe_get("https://www.daangn.com/kr/"):
            print(f"âš ï¸ [{district}] í™ˆí˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨")
            return []

        try:
            # ê²€ìƒ‰ì°½ ì°¾ê¸°
            search_input = self.wait.until(
                EC.presence_of_element_located((By.CSS_SELECTOR, 'input[type="search"]'))
            )
            search_input.clear()
            search_input.send_keys(district)
            time.sleep(1)  # ìë™ì™„ì„± ëŒ€ê¸°

            # ìë™ì™„ì„± ë¦¬ìŠ¤íŠ¸ì˜ ì²« ë²ˆì§¸ í•­ëª© í´ë¦­
            first_item = self.wait.until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, 'ul li a'))
            )
            href = first_item.get_attribute("href")
            first_item.click()
            time.sleep(2)

            # 'ìœ„ì¹˜' ì„¹ì…˜ì˜ ë”ë³´ê¸° í´ë¦­
            more_btn = self.wait.until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, 'button[data-gtm="search_show_more_options"]'))
            )
            more_btn.click()
            time.sleep(1)

            # 'ìœ„ì¹˜' ì„¹ì…˜ì˜ ëª¨ë“  ë™ë„¤ ë§í¬ ì¶”ì¶œ
            location_section = self.wait.until(
                EC.presence_of_element_located((By.XPATH, "//h3[text()='ìœ„ì¹˜']/following-sibling::div[1]"))
            )
            links = location_section.find_elements(By.CSS_SELECTOR, 'a[data-gtm="search_filter"]')
            
            dongs = []
            for a in links:
                try:
                    name = a.text.strip()
                    path = a.get_attribute("href").replace("https://www.daangn.com", "")
                    # í¼ì„¼íŠ¸ ì¸ì½”ë”©ì„ í•œê¸€ë¡œ ë””ì½”ë”©
                    path = urllib.parse.unquote(path)
                    dongs.append({"name": name, "url": path})
                except:
                    continue

            print(f"âœ… [{district}] {len(dongs)}ê°œ ë™ë„¤ ë°œê²¬")
            return dongs

        except Exception as e:
            print(f"âš ï¸ [{district}] ë™ë„¤ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return []

    def process_category(self, region, cat):
        """í•œ ë™ë„¤ì—ì„œ í•œ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§"""
        cat_name = cat["name"].replace("/", "-")  # ìŠ¬ë˜ì‹œë¥¼ í•˜ì´í”ˆìœ¼ë¡œ ë³€ê²½
        region_name = region["name"]
        # region["url"]ì—ì„œ in=... ë¶€ë¶„ë§Œ ì¶”ì¶œ (í•œê¸€)
        m = re.search(r"in=([^&]+)", region["url"])
        if m:
            region_in = m.group(1)
        else:
            region_in = ""
        # cat["url"]ì—ì„œ category_id ì¶”ì¶œ
        m2 = re.search(r"category_id=(\d+)", cat["url"])
        cat_id = m2.group(1) if m2 else "1"
        # in=... ë¶€ë¶„ë§Œ regionì˜ í•œê¸€ë¡œ êµì²´ (ì¸ì½”ë”©í•´ì„œ)
        cat_url = re.sub(r"in=[^&]+", f"in={urllib.parse.quote(region_in)}", cat["url"])
        full_url = "https://www.daangn.com" + cat_url
        print(f"\nâ–¶ [{region_name}][{cat['name']}] í¬ë¡¤ë§ ì‹œì‘")
        
        # ì•ˆì „í•˜ê²Œ í˜ì´ì§€ ë¡œë“œ
        if not self.safe_get(full_url):
            print(f"âš ï¸ [{region_name}][{cat['name']}] í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨, ë‹¤ìŒìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
            return

        # ë™ë„¤ë³„ í´ë” ìƒì„±
        os.makedirs(region_name, exist_ok=True)
        fn = f"daangn_{region_name}_{cat_name}.csv"
        with open(os.path.join(region_name, fn), "w", newline="", encoding="utf-8-sig") as f:
            w = csv.writer(f)
            w.writerow(["ì¹´í…Œê³ ë¦¬","ìƒí’ˆëª…","ê°€ê²©","ì‘ì„±ì¼ì","íŒë§¤ìƒíƒœ","URL"])
            f.flush()

            try:
                # (2) ìŠ¤í¬ë¡¤ + ë”ë³´ê¸° (ìµœëŒ€ 5ë²ˆ)
                click_count = 0
                for _ in range(5):  # ìµœëŒ€ 5ë²ˆ ì‹œë„
                    self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(1)
                    try:
                        btn = self.wait.until(EC.element_to_be_clickable(
                            (By.XPATH, "//button[contains(text(),'ë”ë³´ê¸°')]")
                        ))
                        self.driver.execute_script("arguments[0].scrollIntoView({block:'center'});", btn)
                        btn.click()
                        click_count += 1
                        print(f"     ë”ë³´ê¸° í´ë¦­ ({click_count}/5)")
                        time.sleep(1)
                    except:
                        if click_count == 0:
                            print("     ë”ë³´ê¸° ë²„íŠ¼ ì—†ìŒ")
                        else:
                            print(f"     ë”ë³´ê¸° ë²„íŠ¼ ì—†ìŒ (ì´ {click_count}íšŒ í´ë¦­)")
                        break

                # (3) ëª¨ë“  ìƒí’ˆ ì¹´ë“œ ìˆ˜ì§‘
                cards = self.driver.find_elements(By.CSS_SELECTOR, 'a[data-gtm="search_article"]')
                print(f"   â–¶ [{cat['name']}] {len(cards)}ê°œ ìƒí’ˆ ë°œê²¬")
                
                for idx, card in enumerate(cards, 1):
                    try:
                        name  = card.find_element(By.CSS_SELECTOR, 'span.lm809sh').text.strip()
                        price = card.find_element(By.CSS_SELECTOR, 'span.lm809si').text.strip()
                        time_ = card.find_element(By.TAG_NAME, 'time').text.strip()
                        try:
                            status = card.find_element(By.CSS_SELECTOR,'span.mlbp660').text.strip()
                        except NoSuchElementException:
                            status = "íŒë§¤ì¤‘"
                        
                        w.writerow([cat["name"], name, price, time_, status, card.get_attribute("href")])
                        f.flush()
                        print(f"     {idx}/{len(cards)} [{status}] {name}")
                    except Exception as e:
                        print(f"     âš ï¸ {idx}ë²ˆì§¸ ìƒí’ˆ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                        continue

                print(f"âœ… [{region_name}][{cat['name']}] â†’ {region_name}/{fn}")

            except Exception as e:
                print(f"âš ï¸ [{region_name}][{cat['name']}] ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")

    def run_all_seoul(self):
        """ì„œìš¸ì‹œ ì „ì²´ êµ¬ ìˆœíšŒ"""
        for district in SEOUL_DISTRICTS:
            # êµ¬ì˜ ëª¨ë“  ë™ë„¤ ì •ë³´ ìˆ˜ì§‘
            dongs = self.get_district_dongs(district)
            if not dongs:
                continue

            # ê° ë™ë„¤ë³„ë¡œ ì¹´í…Œê³ ë¦¬ ìˆœíšŒ
            for dong in dongs:
                for cat in categories:
                    self.process_category(dong, cat)

    def close(self):
        try:
            self.driver.quit()
        except:
            pass

crawler = KarrotCrawler()
try:
    crawler.run_all_seoul()
finally:
    crawler.close()
    print("\nğŸ‰ ì„œìš¸ì‹œ ì „ì²´ êµ¬Ã—ë™ë„¤Ã—ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì™„ë£Œ!") 