import json, os, csv, time
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, TimeoutException
import urllib.parse
import re

# â”€â”€â”€ ìºì‹œëœ JSON ì½ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with open("regions_gangnam_remaining.json","r",encoding="utf-8") as f:
    regions = json.load(f)
with open("categories.json","r",encoding="utf-8") as f:
    categories = json.load(f)

class KarrotCrawler:
    def __init__(self):
        opts = uc.ChromeOptions()
        opts.add_argument('--no-sandbox')
        opts.add_argument('--disable-dev-shm-usage')
        opts.add_argument('--disable-blink-features=AutomationControlled')
        opts.add_argument('--disable-infobars')
        opts.add_argument('--disable-extensions')
        opts.add_argument('--start-maximized')
        self.driver = uc.Chrome(options=opts)
        self.wait   = WebDriverWait(self.driver, 15)  # íƒ€ì„ì•„ì›ƒ 15ì´ˆë¡œ ì¦ê°€

    def process_category(self, region, cat):
        """í•œ ë™ë„¤ì—ì„œ í•œ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§"""
        cat_name = cat["name"].replace("/", "-")  # ìŠ¬ë˜ì‹œë¥¼ í•˜ì´í”ˆìœ¼ë¡œ ë³€ê²½
        region_name = region["name"]
        # region["url"]ì—ì„œ in=... ë¶€ë¶„ë§Œ ì¶”ì¶œ (í•œê¸€)
        m = re.search(r"in=([^&]+)", region["url"])
        if m:
            region_in = m.group(1)
        else:
            region_in = ""
        # cat["url"]ì—ì„œ category_id ì¶”ì¶œ
        m2 = re.search(r"category_id=(\d+)", cat["url"])
        cat_id = m2.group(1) if m2 else "1"
        # in=... ë¶€ë¶„ë§Œ regionì˜ í•œê¸€ë¡œ êµì²´ (ì¸ì½”ë”©í•´ì„œ)
        cat_url = re.sub(r"in=[^&]+", f"in={urllib.parse.quote(region_in)}", cat["url"])
        full_url = "https://www.daangn.com" + cat_url
        print(f"\nâ–¶ [{region_name}][{cat['name']}] í¬ë¡¤ë§ ì‹œì‘")
        self.driver.get(full_url)
        time.sleep(2)

        # ë™ë„¤ë³„ í´ë” ìƒì„±
        os.makedirs(region_name, exist_ok=True)
        fn = f"daangn_{region_name}_{cat_name}.csv"
        with open(os.path.join(region_name, fn), "w", newline="", encoding="utf-8-sig") as f:
            w = csv.writer(f)
            w.writerow(["ì¹´í…Œê³ ë¦¬","ìƒí’ˆëª…","ê°€ê²©","ì‘ì„±ì¼ì","íŒë§¤ìƒíƒœ","URL"])
            f.flush()

            try:
                # (2) ìŠ¤í¬ë¡¤ + ë”ë³´ê¸° (ë²„íŠ¼ì´ ì—†ì„ ë•Œê¹Œì§€)
                more_click_count = 0
                while more_click_count < 5:  # ìµœëŒ€ 5íšŒë¡œ ì œí•œ
                    self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(1)
                    try:
                        btn = self.wait.until(EC.element_to_be_clickable(
                            (By.XPATH, "//button[contains(text(),'ë”ë³´ê¸°')]")
                        ))
                        self.driver.execute_script("arguments[0].scrollIntoView({block:'center'});", btn)
                        btn.click()
                        more_click_count += 1
                        print(f"     ë”ë³´ê¸° í´ë¦­ ({more_click_count}/5)")
                        time.sleep(1)
                    except:
                        print(f"     ë”ë³´ê¸° ë²„íŠ¼ ì—†ìŒ (ì´ {more_click_count}íšŒ í´ë¦­)")
                        break

                # (3) ëª¨ë“  ìƒí’ˆ ì¹´ë“œ ìˆ˜ì§‘
                cards = self.driver.find_elements(By.CSS_SELECTOR, 'a[data-gtm="search_article"]')
                print(f"   â–¶ [{cat['name']}] {len(cards)}ê°œ ìƒí’ˆ ë°œê²¬")
                
                for idx, card in enumerate(cards, 1):
                    try:
                        name  = card.find_element(By.CSS_SELECTOR, 'span.lm809sh').text.strip()
                        price = card.find_element(By.CSS_SELECTOR, 'span.lm809si').text.strip()
                        time_ = card.find_element(By.TAG_NAME, 'time').text.strip()
                        try:
                            status = card.find_element(By.CSS_SELECTOR,'span.mlbp660').text.strip()
                        except NoSuchElementException:
                            status = "íŒë§¤ì¤‘"
                        
                        w.writerow([cat["name"], name, price, time_, status, card.get_attribute("href")])
                        f.flush()
                        print(f"     {idx}/{len(cards)} [{status}] {name}")
                    except Exception as e:
                        print(f"     âš ï¸ {idx}ë²ˆì§¸ ìƒí’ˆ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                        continue

                print(f"âœ… [{region_name}][{cat['name']}] â†’ {region_name}/{fn}")

            except Exception as e:
                print(f"âš ï¸ [{region_name}][{cat['name']}] ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")

    def close(self):
        self.driver.quit()

crawler = KarrotCrawler()
try:
    for region in regions:
        for cat in categories:
            crawler.process_category(region, cat)
finally:
    crawler.close()
    print("\nğŸ‰ ëª¨ë“  ë™ë„¤Ã—ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì™„ë£Œ!") 