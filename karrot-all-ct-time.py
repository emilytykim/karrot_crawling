import time, csv, json, os
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException

class KarrotCrawler:
    def __init__(self):
        options = uc.ChromeOptions()
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        self.driver = uc.Chrome(options=options)
        self.wait = WebDriverWait(self.driver, 10)
        
    def process_category(self, category_name, category_url):
        """ì¹´í…Œê³ ë¦¬ë³„ ì²˜ë¦¬"""
        safe_name = category_name.replace("/", "_").replace("\\", "_")
        fn = f"daangn_{safe_name}.csv"
        
        print(f"\nâ–¶ [{category_name}] í¬ë¡¤ë§ ì‹œì‘ â†’ {fn}")
        
        try:
            # ì „ì²´ URL êµ¬ì„± (categories.jsonì—ëŠ” pathë§Œ ì €ì¥ë¨)
            full_url = f"https://www.daangn.com{category_url}"
            self.driver.get(full_url)
            time.sleep(2)
            
            # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ (ìµœëŒ€ 5íšŒ)
            more_click_count = 0
            while more_click_count < 5:
                try:
                    self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(1)
                    btn = self.wait.until(EC.element_to_be_clickable(
                        (By.XPATH, "//button[contains(text(),'ë”ë³´ê¸°')]")
                    ))
                    self.driver.execute_script("arguments[0].scrollIntoView({block:'center'});", btn)
                    btn.click()
                    more_click_count += 1
                    print(f"     ë”ë³´ê¸° í´ë¦­ ({more_click_count}/5íšŒ)")
                    time.sleep(1)
                except:
                    print(f"     ë”ë³´ê¸° ë²„íŠ¼ ì—†ìŒ (ì´ {more_click_count}íšŒ í´ë¦­)")
                    break
            
            # ìƒí’ˆ ì¹´ë“œ ì°¾ê¸°
            cards = self.driver.find_elements(By.CSS_SELECTOR, "a[data-gtm='search_article']")
            print(f"   â–¶ [{category_name}] {len(cards)}ê°œ ìƒí’ˆ ë°œê²¬")
            
            # CSV íŒŒì¼ì— ì €ì¥
            with open(fn, "w", newline="", encoding="utf-8-sig") as f:
                writer = csv.writer(f)
                writer.writerow(["ì¹´í…Œê³ ë¦¬","ìƒí’ˆëª…","ê°€ê²©","ì‘ì„±ì¼ì","íŒë§¤ìƒíƒœ","URL"])
                
                for idx, card in enumerate(cards, 1):
                    try:
                        # ìƒí’ˆëª…
                        name = card.find_element(By.CSS_SELECTOR, "span.lm809sh").text.strip()
                        # ê°€ê²©
                        price = card.find_element(By.CSS_SELECTOR, "span.lm809si").text.strip()
                        # ì‹œê°„
                        time_text = card.find_element(By.TAG_NAME, "time").text.strip()
                        # íŒë§¤ìƒíƒœ
                        try:
                            status = card.find_element(By.CSS_SELECTOR, "span.mlbp660").text.strip()
                        except NoSuchElementException:
                            status = "íŒë§¤ì¤‘"
                        
                        # CSVì— ì“°ê¸°
                        writer.writerow([
                            category_name, name, price, time_text, status, 
                            card.get_attribute("href")
                        ])
                        f.flush()
                        print(f"     {idx}/{len(cards)} [{status}] {name}")
                        
                    except Exception as e:
                        print(f"     âš ï¸ {idx}ë²ˆì§¸ ìƒí’ˆ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                        continue
            
            print(f"âœ… [{category_name}] CSV ìƒì„±ë¨ â†’ {os.path.abspath(fn)}")
            
        except Exception as e:
            print(f"âš ï¸ [{category_name}] ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")

    def close(self):
        self.driver.quit()

def main():
    with open("categories.json", "r", encoding="utf-8") as f:
        categories = json.load(f)
    crawler = KarrotCrawler()
    try:
        for cat in categories:
            crawler.process_category(cat["name"], cat["url"])
    finally:
        crawler.close()
        print("\nğŸ‰ ì¹´í…Œê³ ë¦¬ ìˆœíšŒ + í¬ë¡¤ë§ ì™„ë£Œ!")

if __name__ == "__main__":
    main() 